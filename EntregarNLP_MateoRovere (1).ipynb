{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQpGs6JhQdOk"
      },
      "source": [
        "Instalando las librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mNBra2JRSwkf"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install PyPDF2 chromadb huggingface_hub wikipedia wikidataintegrator transformers sentence-transformers langchain\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 numpy==1.23.4 --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnmZ37OmQgZy"
      },
      "source": [
        "Importando las librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "098DEH2SUP3R"
      },
      "outputs": [],
      "source": [
        "# Interacción con el sistema y usuarios\n",
        "import os\n",
        "import shutil\n",
        "import getpass\n",
        "\n",
        "# Tratamiento de archivos\n",
        "from urllib.request import urlretrieve\n",
        "from zipfile import ZipFile\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Interacción con bases de datos\n",
        "import csv\n",
        "import chromadb\n",
        "import pandas as pd\n",
        "\n",
        "# Descarga de modelos y recursos\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Acceso a Wikidata y Wikipedia\n",
        "import requests\n",
        "import wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXQZwnPrQi-6"
      },
      "source": [
        "Modelo LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3RE2WklUP1Z",
        "outputId": "cc6b56ae-dcf5-4fb8-eb33-f47a2f25c4fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
        "\n",
        "modelo_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,\n",
        "    n_batch=512,\n",
        "    n_gpu_layers=128,\n",
        "    n_ctx=2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de62v5vmQmTq"
      },
      "source": [
        "Modelo de embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1WEqb_qBUPza"
      },
      "outputs": [],
      "source": [
        "modelo_emb = SentenceTransformer('intfloat/multilingual-e5-base')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0C-Lp2jSBpr"
      },
      "source": [
        "Creacion de base de datos vectoriales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tUM8jnX_UPwK"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.Client()\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name='Anatomia')\n",
        "clasificador = chroma_client.get_or_create_collection(name='Clasificador')\n",
        "diccionariov = chroma_client.get_or_create_collection(name='tabla')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yx53JvUQsGC"
      },
      "source": [
        "Importando los libros y el archivo csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rVSe80R0UcmK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0d883497-655f-44d6-9336-09761c3827d6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Destination path '/content/Documentos/CSV/CSV' already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d5c394ab4f64>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0msource_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtarget_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/Documentos/CSV/CSV' already exists"
          ]
        }
      ],
      "source": [
        "github_url = \"https://github.com/Mateorovere/TP2_NLP/archive/main.zip\"\n",
        "zip_file_path, _ = urlretrieve(github_url, \"TP2_NLP.zip\")\n",
        "\n",
        "with ZipFile(zip_file_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "\n",
        "os.remove(zip_file_path)\n",
        "\n",
        "source_dir = \"/content/TP2_NLP-main/Archivos\"\n",
        "target_dir = \"/content/Documentos\"\n",
        "\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(source_dir):\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    target_path = os.path.join(target_dir, filename)\n",
        "    shutil.move(source_path, target_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlFzuQ9XQy9B"
      },
      "source": [
        "Usando RecursiveCharacterTextSplitter para armar los bloques de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XZ3DMwbwUPt5"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import PyPDF2\n",
        "import re\n",
        "\n",
        "output_folder = '/content/'\n",
        "files_list = []\n",
        "index_list = []\n",
        "\n",
        "def split_text_into_parts(text):\n",
        "    # No need for max_length condition, split the text as is\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, separators=['\\n\\n','\\n'])\n",
        "    parts = text_splitter.split_text(text)\n",
        "    return parts\n",
        "\n",
        "file_names = ['/content/Documentos/Libros/LIBRO ANATOMIA (1).pdf',\n",
        "    '/content/Documentos/Libros/LIBRO ANATOMIA (2).pdf',\n",
        "    '/content/Documentos/Libros/LIBRO ANATOMIA (3).pdf']\n",
        "\n",
        "for i, path in enumerate(file_names):\n",
        "    with open(path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "\n",
        "        for page_num in range(num_pages):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text = page.extract_text()\n",
        "            if len(text) < 200:\n",
        "                continue\n",
        "            # Preprocess the text as needed\n",
        "            remove_tabs = re.sub(r'\\t', ' ', text)\n",
        "            clean_text = re.sub(r'-\\n', '', remove_tabs)\n",
        "            # Split the text into parts\n",
        "            text_parts = split_text_into_parts(clean_text)\n",
        "\n",
        "            # Save each part as a separate text file\n",
        "            for part_num, part_text in enumerate(text_parts):\n",
        "                output_file = f'{output_folder}body_{i + 1}page{page_num + 1}part{part_num + 1}.txt'\n",
        "                files_list.append(output_file)\n",
        "                index_list.append(f'{i + 1},{page_num + 1},{part_num + 1}')\n",
        "                with open(output_file, 'w') as f:\n",
        "                    f.write(part_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XerpyViRFxi"
      },
      "source": [
        "Juntando todos los .txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wAzJ2a5jUPqp"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for i, txt in enumerate(files_list):\n",
        "  with open(txt, 'r') as f:\n",
        "    data = f.read().rstrip()\n",
        "  documents.append(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vw3_tpNNRKhK"
      },
      "source": [
        "Vectorizando los textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f4BehEcFUPop"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "textos_array = np.array(documents)\n",
        "\n",
        "embeddings = modelo_emb.encode(textos_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lyf6vSA4UPmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09891afc-d639-4656-d5e8-096a2a260ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.01113987  0.0329801   0.02444133 ... -0.04456066 -0.05338322\n",
            "   0.02405565]\n",
            " [-0.00762356  0.02733346  0.02553917 ... -0.03759247 -0.03872189\n",
            "   0.03295716]\n",
            " [-0.01710617  0.01247476  0.01278696 ... -0.05208471 -0.03355652\n",
            "   0.03258586]\n",
            " ...\n",
            " [ 0.01121867  0.03908517  0.0027669  ... -0.02355399 -0.03583461\n",
            "   0.05515616]\n",
            " [ 0.00294123  0.03731669 -0.01513006 ... -0.04230331 -0.04702177\n",
            "   0.06183393]\n",
            " [ 0.00447942  0.02716055 -0.01747998 ... -0.04605977 -0.04650914\n",
            "   0.06807122]]\n"
          ]
        }
      ],
      "source": [
        "embeddings_list = embeddings.tolist()\n",
        "\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6xeko462UoSy"
      },
      "outputs": [],
      "source": [
        "collection.add(\n",
        "    documents=documents,\n",
        "    ids= index_list,\n",
        "    embeddings=embeddings_list\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vyZC8MEoUoPK"
      },
      "outputs": [],
      "source": [
        "consulta = \"¿Que funciones tiene un vaso sanguineo?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "92tk_HNPUoNR"
      },
      "outputs": [],
      "source": [
        "def search_database(consulta):\n",
        "  embedding_consulta = modelo_emb.encode([consulta]).tolist()\n",
        "  results = collection.query(\n",
        "  query_embeddings=embedding_consulta, # Aquí pasamos el embedding de la consulta\n",
        "  n_results=1 # Traemos el resultado más cercanos\n",
        "  )\n",
        "  return results['documents'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p92Gk-CoUoLh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b79d88-b981-4d50-e0ea-c08024789156"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Anatomía y fisiología del cuerpo humano 138\\npacidad para almacenar de forma dinámica una parte importante \\ndel volumen de sangre. Por ello otra función del sistema venoso es la regulación del volumen circulante sanguíneo. \\nLa presión que existe en el interior de un vaso es la suma']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "results = search_database(consulta)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AkDY97QPfC24"
      },
      "outputs": [],
      "source": [
        "def vectorizar_texto(text):\n",
        "  embedding = modelo_emb.encode([text])\n",
        "  return embedding[0].tolist()\n",
        "\n",
        "def vectorizar_lista_texto(lista_texto):\n",
        "    return [vectorizar_texto(text) for text in lista_texto]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a94JiAKpRaoy"
      },
      "source": [
        "Utilizando el CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0uDCRB6hUoJR"
      },
      "outputs": [],
      "source": [
        "ruta_archivo = '/content/Documentos/CSV/CSV/Sistemas.csv'\n",
        "\n",
        "df_dic = pd.read_csv(ruta_archivo, delimiter=';')\n",
        "\n",
        "palabras = df_dic.iloc[:, 0].tolist()\n",
        "\n",
        "diccionariov.add(\n",
        "    embeddings=vectorizar_lista_texto(palabras),\n",
        "    documents=palabras,\n",
        "    ids=list(map(str, range(len(palabras))))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AEjmiyLSMRK"
      },
      "source": [
        "Clasificador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "YUUlwgzZW91G"
      },
      "outputs": [],
      "source": [
        "lista_consultas= [\"De que esta comupesto el sistema nervioso?\",\n",
        "               'Cuantos órganos tiene el sistema muscular?',\n",
        "               'cuales son las funciones del sistema cardiovascular?',\n",
        "               'cuanto pesa un cerebro?',\n",
        "               'cuanta sangre se bombea en un segundo?',\n",
        "               'de que esta compuesto el sistema excretor?',\n",
        "               'cuanto mide el femur?',\n",
        "               'cuantas respiraciones se hacen en un minuto?',\n",
        "               'dime las funciones del sistema nervioso',\n",
        "               'dime las funciones del sistema endocrino',\n",
        "\n",
        "               '¿Que es la anatomia humana?',\n",
        "               '¿De que esta compuesto un hueso?',\n",
        "               '¿Cuántos huesos hay en el cuerpo humano?',\n",
        "               '¿Qué son las suturas y dónde se encuentran?',\n",
        "               '¿Qué es un tendón y qué función tiene?',\n",
        "               '¿Cómo se produce la contracción muscular?',\n",
        "               '¿Qué son las arterias, las venas y los capilares?',\n",
        "               '¿Cómo se produce la respiración?',\n",
        "               '¿Qué es el intercambio gaseoso?',\n",
        "               '¿Qué es la peristalsis?',\n",
        "\n",
        "               '¿Cuáles son las partes del sistema nervioso?',\n",
        "               '¿Qué son las hormonas?',\n",
        "               '¿Cómo funciona el corazón?',\n",
        "               '¿Qué es la diabetes y cómo se trata?',\n",
        "               '¿Por qué me duele la cabeza cuando estoy cansado?',\n",
        "               '¿Cuáles son los síntomas de la diabetes tipo 2?',\n",
        "               '¿Cómo funciona el sistema inmunológico y cómo nos protege de las enfermedades?',\n",
        "               '¿Qué son los órganos internos y cuál es su función?',\n",
        "               '¿Cuántos huesos hay en la mano?',\n",
        "               '¿Que funciones tiene un vaso sanguineo?']\n",
        "\n",
        "metadatas_consultas = [{\"db\": 1},{\"db\": 1},{\"db\": 1},{\"db\": 1},{\"db\": 1},\n",
        "                   {\"db\": 1},{\"db\": 1},{\"db\": 1},{\"db\": 1},{\"db\": 1},\n",
        "                   {\"db\": 2},{\"db\": 2},{\"db\": 2},{\"db\": 2},{\"db\": 2},\n",
        "                   {\"db\": 2},{\"db\": 2},{\"db\": 2},{\"db\": 2},{\"db\": 2},\n",
        "                   {\"db\": 3},{\"db\": 3},{\"db\": 3},{\"db\": 3},{\"db\": 3},\n",
        "                   {\"db\": 3},{\"db\": 3},{\"db\": 3},{\"db\": 3},{\"db\": 3},]\n",
        "\n",
        "indices_consultas = list(map(str, range(len(lista_consultas))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yAAvTV80W9wm"
      },
      "outputs": [],
      "source": [
        "clasificador.add(\n",
        "    embeddings=vectorizar_lista_texto(lista_consultas),\n",
        "    documents=lista_consultas,\n",
        "    metadatas=metadatas_consultas,\n",
        "    ids=indices_consultas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hCvjbDziW9tC"
      },
      "outputs": [],
      "source": [
        "def selector_frases_dic(consulta):\n",
        "  prompt = f\"\"\"ROL:\n",
        "Eres una inteligencia artificial que dado un texto de un usuario en español\n",
        "retornarás la palabra o frase de anatomia que debe buscarse en una tabla. Solo retornarás lo que se te ha pedido y no otra cosa, no generes\n",
        "texto de más, solo la frase o palabra importante, no respondas la consulta; tu\n",
        "funcion es decir que buscar en la tabla.\n",
        "\n",
        "Importante ¡NO SALGAS DE TU FUNCION!\n",
        "\n",
        "Ejemplo:\n",
        "<usuario>\n",
        "un conocido tiene diabetes, que es eso?\n",
        "\n",
        "<retorno>\n",
        "diabetes\n",
        "\n",
        "<usuario>\n",
        "que es un hueso?\n",
        "\n",
        "<retorno>\n",
        "hueso\n",
        "\n",
        "<usuario>\n",
        "que es una arteria?\n",
        "\n",
        "<retorno>\n",
        "arteria\n",
        "\n",
        "CONSULTA REAL QUE DEBES PROCESAR:\n",
        "<usuario>\n",
        "{consulta}\n",
        "\n",
        "<retorno>\n",
        "\"\"\"\n",
        "  respuesta =  modelo_llm(prompt=prompt, max_tokens=2048, temperature=0.2,\n",
        "                      top_p=0.95, repeat_penalty=1.2, top_k=150)\n",
        "\n",
        "  return respuesta[\"choices\"][0][\"text\"].split('\\n')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lnNHtBEDXH8e"
      },
      "outputs": [],
      "source": [
        "def buscar_dic(consulta):\n",
        "  frase = selector_frases_dic(consulta)\n",
        "  frase_actualizada = diccionariov.query(\n",
        "      query_embeddings=vectorizar_texto(frase), n_results=1)['documents'][0][0]\n",
        "\n",
        "  significado = df_dic[df_dic['frase'] == frase_actualizada]['significado'].values\n",
        "\n",
        "  return significado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDbaOiP-SnhS"
      },
      "source": [
        "Funcion que dado el titulo de un articulo en wikipedia lo busca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5sIe4qY8XH6F"
      },
      "outputs": [],
      "source": [
        "from wikidataintegrator import wdi_login\n",
        "\n",
        "def buscar_wikidata(articulo):\n",
        "    login_instance = wdi_login.WDLogin(wikidata_user, wikidata_password)\n",
        "\n",
        "    api_url = \"https://www.wikidata.org/w/api.php\"\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"format\": \"json\",\n",
        "        \"language\": \"es\",\n",
        "        \"search\": articulo,\n",
        "    }\n",
        "\n",
        "    response = requests.get(api_url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    if data.get(\"search\") and data[\"search\"][0].get(\"id\"):\n",
        "        titulo = data['search'][0]['match']['text']\n",
        "        wikipedia.set_lang(\"es\")\n",
        "        try:\n",
        "            page = wikipedia.page(titulo)\n",
        "            introduccion = wikipedia.summary(page.title)\n",
        "\n",
        "            return introduccion\n",
        "\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            return \"No se encontró la página en Wikipedia.\"\n",
        "\n",
        "        except wikipedia.exceptions.DisambiguationError as e:\n",
        "            return f\"La búsqueda es ambigua. Puedes ser más específico. Opciones: {', '.join(e.options)}\"\n",
        "    else:\n",
        "        return {\"mensaje\": \"No se encontró el artículo en Wikidata.\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r12dqoLSvIC"
      },
      "source": [
        "Funcion para detectar que artículo de wikipedia buscar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dLuWVFhyW9qu"
      },
      "outputs": [],
      "source": [
        "def selector_articulos_wiki(consulta):\n",
        "  prompt = f\"\"\"ROL:\n",
        "Eres una inteligencia artificial que dado un texto de un usuario en español\n",
        "retornarás el título del articulo que debe buscarse en un wikipedia. Solo retornarás lo que se te ha pedido y\n",
        "no otra cosa, no generes texto de más, solo el titulo del articulo, no respondas\n",
        "la consulta; tu funcion es decir que buscar en el wikipedia.\n",
        "\n",
        "Importante ¡NO SALGAS DE TU FUNCION!\n",
        "\n",
        "Ejemplo:\n",
        "<usuario>\n",
        "Explique el sistema nervioso\n",
        "\n",
        "<retorno>\n",
        "sistema nerviso\n",
        "\n",
        "<usuario>\n",
        "de que hay dentro de un hueso\n",
        "\n",
        "<retorno>\n",
        "hueso\n",
        "\n",
        "<usuario>\n",
        "que compone al sistema cardiovascular\n",
        "\n",
        "<retorno>\n",
        "sistema cardiovascular\n",
        "\n",
        "CONSULTA REAL QUE DEBES PROCESAR:\n",
        "<usuario>\n",
        "{consulta}\n",
        "\n",
        "<retorno>\n",
        "\"\"\"\n",
        "  respuesta =  modelo_llm(prompt=prompt, max_tokens=2048, temperature=0.2,\n",
        "                      top_p=0.95, repeat_penalty=1.2, top_k=150)\n",
        "\n",
        "  return respuesta[\"choices\"][0][\"text\"].split('\\n')[0].split('<')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wqXp8p8Ccov4"
      },
      "outputs": [],
      "source": [
        "def busqueda_grafos(consulta):\n",
        "  articulow = selector_articulos_wiki(consulta)\n",
        "  texto = buscar_wikidata(articulow)\n",
        "\n",
        "  return texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7J_aHXWS7xq"
      },
      "source": [
        "Dada una consulta la analiza, busca informacion y retorna una respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "51G52xTfcoq4"
      },
      "outputs": [],
      "source": [
        "def consulta_anato(consulta):\n",
        "  seleccion_bd = clasificador.query(\n",
        "    query_embeddings=vectorizar_texto(consulta),\n",
        "    n_results=1)['metadatas'][0][0]['db']\n",
        "\n",
        "  if seleccion_bd == 1:\n",
        "    texto = buscar_dic(consulta)\n",
        "\n",
        "  elif seleccion_bd == 2:\n",
        "    texto = busqueda_grafos(consulta)\n",
        "\n",
        "  else:\n",
        "    articulos = collection.query(\n",
        "      query_embeddings=vectorizar_texto(consulta),\n",
        "      n_results=5)['documents'][0]\n",
        "\n",
        "    texto = '\\n'.join(articulos)\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "Eres un consultor en anatomia humana que dada la consulta del Usuario y según\n",
        "la información que dispone da la respuesta mas detallada y completa posible y\n",
        "en español, siempre responde con lo que dice la documentación de manera\n",
        "adaptada a personas sin conocimientos sobre anatomia y nunca hace preguntas.\n",
        "\n",
        "Tu función es responder solo lo que dice el Consultor y nada más.\n",
        "\n",
        "Información: {texto}\n",
        "\n",
        "Usuario: {consulta}\n",
        "\n",
        "Consultor: Buenos días/tardes Usuario(a), como consultor en anatomia, puedo informarte que\"\"\"\n",
        "\n",
        "  respuesta_llm = modelo_llm(prompt=prompt, max_tokens=2048,\n",
        "                           temperature=0.8, top_p=0.95,\n",
        "                           repeat_penalty=1.2, top_k=150)[\"choices\"][0][\"text\"]\n",
        "\n",
        "  return respuesta_llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "S29jA6PIcooZ"
      },
      "outputs": [],
      "source": [
        "def actualizar_clasificador(consulta):\n",
        "  if str(input(\"\\n¿Estas satisfecho con la respuesta? s/n\\n\")).lower() != \"n\":\n",
        "    return\n",
        "\n",
        "  mejor_db = str(input(\"\"\"\n",
        "¿En donde buscarías inconsultaformacion para responder tu ?\n",
        "\n",
        "  1. tabla.\n",
        "  2. Wikipedia.\n",
        "  3. libros de Anatomia.\n",
        "  \"\"\"))\n",
        "\n",
        "  clasificador.add(\n",
        "    embeddings=vectorizar_texto(consulta),\n",
        "    documents=[consulta],\n",
        "    metadatas=[{\"db\": mejor_db}],\n",
        "    ids=[str(clasificador.count())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTaSjyMITEQC"
      },
      "source": [
        "Ingrese sus datos:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gJ0ksMaAiMmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf299007-ec81-40de-dd14-b2edab0bdd80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ingrese usuario de Wikidata: ··········\n",
            "Ingrese contraseña de Wikidata: ··········\n"
          ]
        }
      ],
      "source": [
        "wikidata_user = getpass.getpass('Ingrese usuario de Wikidata: ')\n",
        "wikidata_password = getpass.getpass('Ingrese contraseña de Wikidata: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-o-7QZDiMhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b46598-c7bb-4a12-965c-3539bd51d901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Ingrese su consulta ('q' para salir): que es la anatomia\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buenos días/tardes Usuario(a), como consultor en anatomia humana, puedo informarte que  la anatomía \n",
            "es una ciencia que estudia la estructura de los seres vivos. Se divide en tres ramas principales: an\n",
            "atomía descriptiva, funcional y quirúrgica. La anatomía se basa en el examen descriptivo de los orga\n",
            "nismos vivos para comprender su forma, topografía, ubicación y relación entre sí de sus componentes \n",
            "orgánicos. Además, la anatomía también se relaciona con la fisiología para entender cómo estos compo\n",
            "nentes trabajan juntos para mantener las funciones corporales básicas. Como Usuario(a), esto te ayud\n",
            "ará a comprender mejor cómo los diferentes órganos y sistemas del cuerpo humano interactúan entre sí\n",
            " para mantener la salud y el bienestar. ¿Hay alguna pregunta específica que desees hacerme sobre la \n",
            "anatomía?\n",
            "\n",
            "\n",
            "¿Estas satisfecho con la respuesta? s/n\n",
            "s\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Ingrese su consulta ('q' para salir): de que estan compuestos los huesos?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buenos días/tardes Usuario(a), como consultor en anatomia humana, puedo informarte que  los huesos e\n",
            "stán compuestos por varias partes. En primer lugar, tenemos el tejido óseo, un tipo de conectivo fir\n",
            "me y duro que está compuesto por células llamadas osteocitos y componentes extracelulares calcificad\n",
            "os. Además, los huesos tienen una cubierta superficial de tejido conectivo fibroso llamado periostio\n",
            " y presentan superficies articulares revestidas por tejido cartilaginoso. También hay un espacio int\n",
            "erior lleno de médula ósea, que está formada por tejidos blandos que incluyen células hematopoyética\n",
            "s y adiposas. Por último, los huesos contienen vasos sanguíneos y nervios que les proporcionan oxíge\n",
            "no e innervan su estructura. Espero que esta información sea útil para ti. ¿Hay alguna otra pregunta\n",
            "?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    longitud_maxima = 100\n",
        "\n",
        "    print('\\n' + '-' * longitud_maxima + '\\n')\n",
        "\n",
        "    try:\n",
        "        consulta = str(input(\"Ingrese su consulta ('q' para salir): \"))\n",
        "        print()\n",
        "        if consulta.lower() == 'q':\n",
        "          print()\n",
        "          print('Saliendo...')\n",
        "          break\n",
        "    except Exception as e:\n",
        "        print(f\"Error al leer la entrada del usuario: {e}\")\n",
        "        continue\n",
        "\n",
        "    resultado = \"\\n\\nBuenos días/tardes Usuario(a), como consultor en anatomia humana, puedo informarte que \" + consulta_anato(consulta)\n",
        "\n",
        "    lineas_resultado = resultado.split('\\n')\n",
        "\n",
        "    resultado_segmentado = [linea[i:i+longitud_maxima] for linea in lineas_resultado for i in range(0, len(linea), longitud_maxima)]\n",
        "\n",
        "    for linea in resultado_segmentado:\n",
        "        print(linea)\n",
        "\n",
        "    print()\n",
        "\n",
        "    actualizar_clasificador(consulta)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}